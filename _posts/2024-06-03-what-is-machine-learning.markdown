---
layout: post
title: "What is machine learning?"
date: 2024-06-03 19:22:00 +0100
categories: Machine Learning
author: Luigi Assenza
comments_id: 2
image: /images/neural_network.png
---
<style>
  .footer-heading, .p-name {
    display: none;
  }
</style>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

A definition of machine learning that often we see is that:
> Machine learning is the computerâ€™s ability to learn without being explicitly programmed.

This definition, though it covers the big picture of machine learning, seems to be a bit too abstract. Let's start to dig a bit deeper and be more concrete, looking first at its goal.

# The goal
Machine learning's objective is to produce an outcome given an input, be this a prediction or a classification. The input is the data we provide, the data we use to "feed" our computer and from which it will "learn". But the data itself is not enough; we also need to provide the technology necessary to produce the output: this is done through a mathematical function.

# The technology: a mathematical function
A mathematical function tranforms the input into an output. The input is represented by the value/s of its independent variable/s and the output is the value of its dependent variable. In order to produce the value of the dependent variable we need to determine what type of function to use: we need to determine its "shape" if you prefer.

# The function type (shape)
The type of function, essentially its shape if we want to be more visual, is determined by its parameters, and how these are combined with its independent variables. For example, a linear function (like the one used in a simple linear regression) has two parameters, $$m$$ (the slope) and $$b$$ (the intercept or bias in machine learning language), combined in the following way:

$$y = mx + b$$

In this example we only have one variable. But the input can contain as many variables we like and need. So, if we have two variables we get a plane and in case of more than two variables, we get a hyperplane:

$$y = m_1x_1 + m_2x_2 + \dotsc + m_nx_n + b$$

In matrix notation, the generic hyperplane (multiple linear regression) can be expressed as:

$$
y =
\begin{bmatrix}
m_1 & m_2 & \dotsc & m_n
\end{bmatrix} 
\times
\begin{bmatrix}
x_1\\
x_2\\
\dotsc\\
x_n
\end{bmatrix}
+ b
= m^Tx + b
$$

Functions come in all sorts of shapes. Even something as complex as a deep neural network is a mathematical function. A big one, but a function. 
Let's consider a simple neural network with two neurons in the input layer, two neurons in the hidden layer and one neuron in the otput layer:

<figure>
    <img src="{{ page.image }}"/>
    <figcaption>Figure 1 - A simple neural network</figcaption>
</figure>

In the input lavyer (layer $$0$$) we have:

$$
x =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
a_1^0 \\
a_2^0 \\
\end{bmatrix}
=
a^0
$$

The $$a_i^0$$ elements represent the values generated by the activation function. For the input layer, these values are exactly the same as the input values (the values of the independent variables).

In the hidden lavyer (layer $$1$$) we have:

$$
W^1 =
\begin{bmatrix}
w_{11}^1 & w_{12}^1 \\
w_{21}^1 & w_{22}^1 \\
\end{bmatrix}
,
Z^1=
\begin{bmatrix}
z_1^1 \\
z_2^1 \\
\end{bmatrix}
,
b^1
,
A^1 =
\begin{bmatrix}
a_1^1 \\
a_2^1 \\
\end{bmatrix}
$$

Where:
1. $$W^1$$ is the weight matrix;
2. $$Z^1$$ is the vector of the linear combination of the output values of the previous layer (layer $$0$$), the weights and the bias;
3. $$b^1$$ is the bias (for simplicity it's equal for every neuron);
4. $$A^1$$ is the output vector of layer one which will feed the next layer (layer $$2$$).

In the final output layer we have:

$$
W^2 =
\begin{bmatrix}
w_{11}^2 \\
w_{21}^2 \\
\end{bmatrix}
,
Z^2 =
\begin{bmatrix}
z_1^2
\end{bmatrix}
,
b^2
,
A^2 = 
\begin{bmatrix}
a_1^2
\end{bmatrix}
$$

Where:
1. $$W^2$$ is the weigth vector;
2. $$Z^2$$ is the vector (only one value in this case) of the linear combination of the output values of the previous layer (layer $$1$$), the weights and the bias;
3. $$b^2$$ is the bias;
4. $$A^2$$ is the output vector (only one value in this case): it is the prediction of the neural network.

If we use our simple neural network to compute our prediction (feed forward), we get:

$$
Z^1 = W^{1T}A^0 + b^1 \\
A^1 = \sigma(Z^1) \\
Z^2 = W^{2T}A^1 + b^2 \\
A^2 = \sigma(Z^2)
$$

where $$\sigma(x)$$ is the activation fuction.

If we express all the different computations represented by the matrix notation in one single formula, we get:

$$
a_1^2 = 
    \sigma^2\Bigg(
        w_1^2
        \sigma^1\left(
            w_{11}^1x_1 + w_{21}^1 + b^1
        \right) 
        + 
        w_2^2
        \sigma^1\left(
            w_{12}^1x_1 + w_{22}^1 + b^1
        \right)
        +
        b^2
    \Bigg)
$$

If, for example, we choose the activation function $$\sigma(x)$$ to be the _ReLU_ function:

$$f(x) = max(x, 0) = \frac{x + |x|}{2}$$

for both the hidden layer ($$\sigma^1$$) and the output layer ($$\sigma^2$$), then we have:

$$
a_1^2 = 
    max\Bigg(
        w_1^2
        max\left(
            w_{11}^1x_1 + w_{21}^1 + b^1,
            0
        \right) 
        + 
        w_2^2
        max\left(
            w_{12}^1x_1 + w_{22}^1 + b^1,
            0
        \right)
        +
        b^2,
        0
    \Bigg)
$$

After choosing the type of function, we then need to determine which specific function we want to use. In order to do that, we need to assign a specific value to all its parameters. In our previous examples, these are

$$m, b$$

in case of the linear regression, and 

$$w_{11}^1, w_{12}^1, w_{21}^1, w_{22}^1, b^1, w_1^2, w_2^2, b^2$$

in case of the neural network.\
 In other words we need to find the best function.

# The best function
But what do we need by best function?
 
The best function is the function that produces the best value. In order to reach it, we need to define what is the value we are aming at. Once we know that, we'll try to get closer and closer.

To do so, we use something called loss function (sometimes also called cost function or objective function).

# The loss function

The loss function, as the name implies, is something negative, something that we want to minimise. In a mathematical term, the goal is to reach the lowest point (a global mininum).

The process of finding those global minima is called optimisation. The lowest point is reached when the derivative of the function (or its gradient if the function has two or more variables) is zero (provided that the function is convex).

For simple functions, like our simple linear regression, the global mininum can be calculated by a closed formula: the derivative of the function.
For more complex functions, like a neural netwok, finding a closed formula becomes impractical. In these situations, an iterative process is used.

# The iterative process

The best iterative process is the gradient descent. In the gradient descent process, we iteratively reduce the value of the loss function until the lowest point is reached (or when we are very close to it).

In order to make the iterative process work, hyperparameters are used.

# Hyperparameters
Unlike the (model) parameters which are used by the machine learning model (the mathematical model), hyperparameters instead are not used directly by the model. They are simply used in the optimisation process to build the model.

Example of hyperparameters are:
1. the maxiumum number of iterations to perform before the algorithm stops;
2. the learning rate, that is the step done by the gradient descent algorithm;
3. the number of neighbours ($$k$$) in the k-nearest neighbour algorithm.

# The machine has learned
Once the best (model) parameters are calculated, the machine (or the computer if you like) has "learned" from past experiences (the input data we used to feed our model).

{% if page.comments_id %}
    {% include comments.html issue_id=page.comments_id %}
{% endif %}