---
layout: post
title: "What is machine learning?"
date: 2024-06-03 19:22:00 +0100
categories: Machine Learning
author: Luigi Assenza
comments_id: 2
image: /images/neural_network.png
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

A definition of machine learning that often we see if that it is *the computer’s ability to learn without being explicitly programmed*.

This definition, though correct, seems to be a bit too much abastract. To dig a bit deeper, let's look first at what is its goal: to produce a outcome given an input, be this a prediction or a classification.

In order to reach its goal it then needs an input: it's the data we provide, from which the computer will "learn". But of course it's not enough to provide data, we also need to provide a way, a technoloyg, to transform the input data into an ouput: this technology is a mathematical function.

A mathematical function tranforms an input into an output. The input are the values of its independent variables and the output is the value of its dependent variable. In order to produce the value of the dependent variable given the values of the independent variable, we need to determine what type of variable is. We need to determine its "shape".

The type of function, or its shape if you like, is determined by its parameters, and how these are combined with its independent variables. For example, a liner function (like the one used in a linear regression model) has two parameters, $$m$$ (the slope) and $$b$$ (the intercept, the bias in machine learning language) combined in the following way:

$$y = mx + b$$

In this example we only have one variable. But the input can contain as many variables we like and need. So, in case we have more than one variable, we would have a hyperplane (if we have two variables usually it's said to be a plane):

$$y = m_1x_1 + m_2x_2 + \dotsc + m_nx_n + b$$

In matrix notation, the hyperplane can be expressed as:

$$
y =
\begin{bmatrix}
m_1 & m_2 & \dotsc & m_n
\end{bmatrix} 
\times
\begin{bmatrix}
x_1\\
x_2\\
\dotsc\\
x_n
\end{bmatrix}
+ b
= m^Tx + b
$$

Functions can come in all shapes and forms. Even something as complex as a deep neural network model is a mathematical function. A big one, but a function.

Even deep learning model is a function. A big one but a function.
Let's consider a simple neural network with two neurons in the input layer, two neurons in the hidden layer and one neuron in the otput layer:

<figure>
    <img src="{{ page.image }}"/>
    <figcaption>Figure 1 - A simple neural network</figcaption>
</figure>

The function of this simple neural network can be built starting from its matrix elements.

In the input lavyer (layer $$0$$) we have:

$$
x =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
a_1^0 \\
a_2^0 \\
\end{bmatrix}
=
a^0
$$

The $$a_i^0$$ elements represent the values generated by the activation function. For the input layer, these values are exactly the same as the input values.\\
In the hidden lavyer (layer $$1$$) we have:

$$
W^1 =
\begin{bmatrix}
w_{11}^1 & w_{12}^1 \\
w_{21}^1 & w_{22}^1 \\
\end{bmatrix}
,
Z^1=
\begin{bmatrix}
z_1^1 \\
z_2^1 \\
\end{bmatrix}
,
b^1
,
A^1 =
\begin{bmatrix}
a_1^1 \\
a_2^1 \\
\end{bmatrix}
$$

Where:
1. $$W^1$$ is the weight matrix
2. $$Z^1$$ is the vector of the linear combination of the output values of the previous layer (layer $$0$$), the weights and the bias
3. $$b^1$$ is the bias (for simplicity it's equal for every neuron)
4. $$A^1$$ is the output vector of layer one which will feed next layer (layer $$2$$).

In the final output layer we have:

$$
W^2 =
\begin{bmatrix}
w_{11}^2 \\
w_{21}^2 \\
\end{bmatrix}
,
Z^2 =
\begin{bmatrix}
z_1^2
\end{bmatrix}
,
b^2
,
A^2 = 
\begin{bmatrix}
a_1^2
\end{bmatrix}
$$

Where:
1. $$W^2$$ is the weigth vector
2. $$Z^2$$ is the vector (only one value in this case) of the linear combination of hte output values of the previous layer (layer $$1$$), the weights and the bias
3. $$b^2$$ is the bias
4. $$A^2$$ is the output vector (only one value in this case): it is the model prediction

If we compute the neural network we get:

$$
Z^1 = W^{1T}A^0 + b^1 \\
A^1 = \sigma(Z^1) \\
Z^2 = W^{2T}A^1 + b^2 \\
A^2 = \sigma(Z^2)
$$

$$\sigma(x)$$ is the activation fuction.

If we express all the different computations represented by the matrix notation in one single formula, we get:

$$
a_1^2 = 
    \sigma^2\Bigg(
        w_1^2
        \sigma^1\left(
            w_{11}^1x_1 + w_{21}^1 + b^1
        \right) 
        + 
        w_2^2
        \sigma^1\left(
            w_{12}^1x_1 + w_{22}^1 + b^1
        \right)
        +
        b^2
    \Bigg)
$$

If we choose the activation function $$\sigma()$$ to be the `ReLU` function:

$$f(x) = max(x, 0)$$

for both $$\sigma^1$$ and $$\sigma^2$$, then we have:

$$
a_1^2 = 
    max\Bigg(
        w_1^2
        max\left(
            w_{11}^1x_1 + w_{21}^1 + b^1,
            0
        \right) 
        + 
        w_2^2
        max\left(
            w_{12}^1x_1 + w_{22}^1 + b^1,
            0
        \right)
        +
        b^2,
        0
    \Bigg)
$$

In this case the parameters to adjust are:

$$w_{11}^1, w_{12}^1, w_{21}^1, w_{22}^1, b^1, w_1^2, w_2^2, b^2$$
 
But what do we mean by the best values?

In order to have a best value we need to determine what value we are aiming at. Once we know what the value is, we'll try to get closer and closer.

The value we are looking for is determined by the so called loss function (sometimes also called cost function or objective function).

The loss function, as the name implies, is something negative, something that we need to minimise. In a mathematical term, the goal is to reach the lowest point (a global mininum).

This process of optimisation aims at finding those function parameters such that the value of the loss function is minimal.

The best values are determined by the input values we use. We iteratively feed the input value trying to reduce the value of the loss function at each iteration untill a minimum is reached.

In this interation process the machine (our algorithm) “learns” from past experiences (the data we feed our model with).

In the iteration process we use hyperparameters: unlike the (model) paramters which are used by the machine learning model (the mathematical model) then are those that define the model, used for prediction once the model has been optimised, hyperparameters instead are not used directly by the mathematical model; they are indirectly used to build the mathematical model.

Example of hyperparameters are:
1. the maxiumum number of iterations to perform before the algorithm stops;
2. the learning rate, that is the step done by the gradient descent algorithm;
3. the number of neighbours ($$k$$) in the k-nearest neighbour algorithm.

{% if page.comments_id %}
    {% include comments.html issue_id=page.comments_id %}
{% endif %}